[{"uri":"/","title":"AWS DataLab 智能货架","tags":[],"description":"","content":"Author\n Haoran Lv (AWS GCR Applied Scientist)  概述 本次workshop分为几个部分\n 背景介绍 - what is Object Detection？ 基于Amazon SageMaker的YOLOv5模型训练动手实验  模型训练 模型部署   基于Amazon SageMaker的Os2d模型使用和部署  模型测试 模型部署    本次 workshop 前提 本次 workshop 建议在 US-WEST-2 Region 使用。为了演示方便，所以本 workshop 所有的演示都会以US-WEST-2 Region 为例。\n"},{"uri":"/01introduction.html","title":"智能货架POC","tags":[],"description":"","content":"目标检测 目标检测（Object Detection）是计算机视觉领域的基本任务之一，学术界已有将近二十年的研究历史。近些年随着深度学习技术的火热发展，目标检测算法也从基于手工特征的传统算法转向了基于深度神经网络的检测技术。该任务旨在提取出图片中的目标位置以及目标类别。\n商超场景下的目标检测 而商超场景下的目标检测面临诸多挑战：品类繁多，标注困难；目标密集，检测困难；上新速度快，模型迭代困难。 在该场景下，本方案旨在挖掘传统目标检测框架和小样本目标检测框架的潜力，充分对比两者的优劣并给出应用建议。\n"},{"uri":"/01introduction/100algorithm.html","title":"1.1 算法概述","tags":[],"description":"","content":"使用场景 根据生成检测框方法不同可以分为Anchor Free方法和Anchor Based方法，而Anchor Based又可以进一步分为One-Stage方法和Two-Stage方法。\n One-Stage：Yolo、SSD、RetinaNet。 Two-Stage：RCNN、Faster RCNN、FPN。  随着2012年AlexNet的出现，越来越多的领域被深度学习屠榜，而目标检测这个停滞多年的视觉领域也重新焕发的活力。目标检测的研究成果在计算机视觉三大顶会（CVPR/ICCV/ECCV）的占比也逐年增加。\n本workshop主要覆盖以下几个算法，对比结果如下\n Pegasus BART  "},{"uri":"/01introduction/cnn.html","title":"1.2 卷积神经网络(CNN)及其理解","tags":[],"description":"","content":"卷积神经网络基础 计算机眼中的图像： 单层卷积操作： 卷积神经网络(Convolutional Neural Network, CNN)： 通过可视化理解卷积神经网络 浅层卷积核以及特征图可视化：底层卷积层的卷积核负责提取低级特征：如颜色，纹理等等 层数加深，特征不断精细化： 深层卷积核以及特征图可视化：最终高层卷积核所对应的特征十分具体且方便分类\nCNN学习到的特征呈现分层特性，底层是一些边缘角点以及颜色的抽象特征，越到高层则越呈现出具体的特征，这一过程正与人类视觉系统类似。故更多的模型向更深的卷积层发展，这也是深度学习的“深”的确切含义\n"},{"uri":"/01introduction/300metrics.html","title":"1.3 评估指标","tags":[],"description":"","content":"目标检测由于涉及到多个子任务：定位，分类。故其评价方法较为复杂，本章从交并比，准确率，精度，召回率，FPR, F1-Score, PR曲线，ROC曲线，AP的值，AUC的值以及很重要的mAP指标，模型的检测速度和非极大值抑制的相关方面来学习下目标检测中的评价指标。\n交并比 — IoU 交并比IoU是英文intersection over union的简写，意思是检测结果的矩形框与样本标注的矩形框的交集与并集的比值。如下图： 上图中假设A为模型的检测结果，B为Ground Truth即样本的标注结果，那么A与B相交的区域即为A∩B，而A与B的并集即为AB共有的区域A∪B,那么IoU的计算公式即为: IoU = (A∩B) / (A∪B)\n一般情况下对于检测框的判定都会存在一个阈值，也就是IoU的阈值，一般可以设置当IoU的值大于0.5的时候，则可认为检测到目标物体。\n准确率/精度/召回率/FPR/F1指标 以上五个指标都离不开下列定义：\n 预测值为正例，记为P（Positive） 预测值为反例，记为N（Negative） 预测值与真实值相同，记为T（True） 预测值与真实值相反，记为F（False）  准确率 准确率accuracy是我们最常见的评价指标，这个很容易理解，就是被分对的样本数除以所有的样本数，通常来说，正确率越高，分类器越好，如下：\n accuracy = (TP+TN)/(TP+TN+FP+FN)  上公式中的TP+TN即为所有的正确预测为正样本的数据与正确预测为负样本的数据的总和，TP+TN+FP+FN即为总样本的个数。\n精度 精度precision是从预测结果的角度来统计的，是说预测为正样本的数据中，有多少个是真正的正样本，即“找的对”的比例，如下：\n precision = TP/( TP+FP)  上公式中的TP+FP即为所有的预测为正样本的数据，TP即为预测正确的正样本个数。\n召回率/TPR 召回率recall和TPR(灵敏度(true positive rate))是一个概念，都是从真实的样本集来统计的，是说在总的正样本中，模型找回了多少个正样本，即“找的全”的比例，如下：\n recall/TPR = TP/(TP+FN)  上公式中的TP+FN即为所有真正为正样本的数据，而TP为预测正确的正样本个数。\nFPR FPR(false positive rate)，它是指实际负例中，错误的判断为正例的比例，这个值往往越小越好，如下：\n FPR = FP/(FP+TN)  其中，FP+TN即为实际样本中所有负样本的总和，而FP则是指判断为正样本的负样本。\nF1-Score F1分数(F1-score)是分类问题的一个衡量指标。F1分数认为召回率和精度同等重要, 一些多分类问题的机器学习竞赛，常常将F1-score作为最终测评的方法。它是精确率和召回率的调和平均数，最大为1，最小为0。计算公式如下：\n F1 = 2TP/(2TP+FP+FN)  此外还有F2分数和F0.5分数。F2分数认为召回率的重要程度是精度的2倍，而F0.5分数认为召回率的重要程度是精度的一半。计算公式为：\n更一般地，我们可以定义Fc（precision和recall权重可调的F1 score）:\n Fc = ((1+c*c)*precision*recall) / (c*c*precision + recall)  PR曲线—AP值/ROC曲线-AUC值 上面学习了关于精度，召回率，FPR，和F1-Score的知识，但是通常，只有那些往往不能够直观的反应模型性能，所以就有了PR曲线，ROC曲线，AUC值。\nPR曲线和AP的值 PR曲线，就是precision和recall的曲线，PR曲线中precision为纵坐标，recall为横坐标,如下图：  评估能力  那么PR曲线如何评估模型的性能呢？从图上理解，如果模型的精度越高，召回率越高，那么模型的性能越好。也就是说PR曲线下面的面积越大，模型的性能越好。绘制的时候也是设定不同的分类阈值来获得对应的坐标，从而画出曲线。\n 优缺点  PR曲线反映了分类器对正例的识别准确程度和对正例的覆盖能力之间的权衡。\nPR曲线有一个缺点就是会受到正负样本比例的影响。比如当负样本增加10倍后，在racall不变的情况下，必然召回了更多的负样本，所以精度就会大幅下降，所以PR曲线对正负样本分布比较敏感。对于不同正负样本比例的测试集，PR曲线的变化就会非常大。\n 平均准确率AP  AP即Average Precision，称为平均准确率，是对不同召回率点上的准确率进行平均，在PR曲线图上表现为PR曲线下面的面积。AP的值越大，则说明模型的平均准确率越高。\nROC曲线和AUC值 ROC的全称是Receiver Operating Characteristic Curve，中文名字叫“受试者工作特征曲线”，对于ROC来说，横坐标就是FPR，而纵坐标就是TPR，因此可以想见，当TPR越大，而FPR越小时，说明分类结果是较好的。如下图：  优缺点  ROC曲线有个很好的特性，当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。\nROC曲线可以反映二分类器的总体分类性能，但是无法直接从图中识别出分类最好的阈值，事实上最好的阈值也是视具体的场景所定。ROC曲线一定在y=x之上，否则就是一个不好的分类器。\n AUC  AUC是Area under curve的首字母缩写，即ROC曲线下的面积，介于0和1之间。计算方式即为ROC曲线的微积分值，其物理意义可以表示为：随机给定一正一负两个样本，将正样本排在负样本之前的概率，因此AUC越大，说明正样本越有可能被排在负样本之前，即正样本分类结果越好。\n平均精度均值 — mAP mAP是英文mean average precision的缩写，意思是平均精度均值，这个词听起来有些拗口，我们来仔细捋一捋。上面我们知道了什么是AP，AP就是PR曲线下面的面积（如下图），是指不同召回率下的精度的平均值。\n然而，在目标检测中，一个模型通常会检测很多种物体，那么每一类都能绘制一个PR曲线，进而计算出一个AP值。那么多个类别的AP值的平均就是mAP\nmAP衡量的是学出的模型在所有类别上的好坏，是目标检测中一个最为重要的指标，一般看论文或者评估一个目标检测模型，都会看这个值，这个值是在0-1直接，越大越好。\n一般来说mAP针对整个数据集而言的，AP针对数据集中某一个类别而言的，而percision和recall针对单张图片某一类别的。\n模型的检测速度 检测速度，这个很好理解，简单的说就是一秒钟能够检测多少张图片。不同的目标检测技术往往会有不同的mAP和检测速度，如下图（后面我们将逐一学习）： 目标检测技术的很多实际应用在准确度和速度上都有很高的要求，如果不计速度性能指标，只注重准确度表现的突破，但其代价是更高的计算复杂度和更多内存需求，对于全面行业部署而言，可扩展性仍是一个悬而未决的问题。\n在实际问题中，通常需要综合考虑mAP和检测速度等因素。\n非极大值抑制（NMS） 非极大值抑制虽然一般不作评价指标，但是也是目标检测中一个很重要的步骤，因为下期就要步入经典模型的介绍了，所以这里随着评价指标简单介绍下。\n 单个预测目标  NMS的英文为Non-Maximum Suppression,就是在预测的结果框和相应的置信度中找到置信度比较高的bounding box。对于有重叠在一起的预测框，如果和当前最高分的候选框重叠面积IoU大于一定的阈值的时候，就将其删除，而只保留得分最高的那个。如下图：  计算步骤：  1). NMS计算出每一个bounding box的面积，然后根据置信度进行排序，把置信度最大的bounding box作为队列中首个要比较的对象；\n2). 计算其余bounding box与当前最大score的IoU，去除IoU大于设定的阈值的bounding box，保留小的IoU预测框；\n3). 然后重复上面的过程，直至候选bounding box为空。\n 多个预测目标  当存在多目标预测时，如下图，先选取置信度最大的候选框B1，然后根据IoU阈值来去除B1候选框周围的框。然后再选取置信度第二大的候选框B2,再根据IoU阈值去掉B2候选框周围的框。 "},{"uri":"/os2d/0501train.html","title":"Os2d模型测试","tags":[],"description":"","content":"我们现在会用sagemaker进行一个Os2d的模型设置，使用ML.P3.2xlarge机型。 打开文件 deploy_models/os2d/demo.ipynb，逐行运行以实现测试。\n环境配置 使用SageMaker原生的pytorch_latest_p37即可，无需额外配置环境。\n模型以及数据准备 %cd ./deploy_models/os2d !aws s3 cp s3://lhr/os2d_v2-train.pth ./models/ !aws s3 cp s3://lhr/classes.zip ./data/demo/ !aws s3 cp s3://lhr/5.jpg ./data/demo/ %cd ./data/demo !unzip classes.zip \u0026gt; /dev/null %cd .. %cd .. 模型测试 接下来我们进行测试，顺着demo.ipynb往下进行即可，整个逻辑即为使用classes文件夹内的class图像在5.jpg内进行特征匹配并实现目标检测，最终的测试结果如下：\n到这里，就完成了Os2d的测试过程。\n"},{"uri":"/yolo/0501train.html","title":"YOLOv5模型训练","tags":[],"description":"","content":"我们现在会用sagemaker进行一个YOLOv5模型的本地训练，使用ML.P3.2xlarge机型。 打开文件 deploy_models/yolov5/demo.ipynb，逐行运行。\n环境配置 首先下载代码\ncd SageMaker git clone git://github.com/ultralytics/yolov5 cd yolov5 pip install -qr requirements.txt # install dependencies import torch from IPython.display import Image, clear_output # to display images clear_output() print(f\u0026quot;Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\u0026quot;) 输出\n Setup complete. Using torch 1.9.1+cu102 (Tesla V100-SXM2-16GB)  即代表环境配置完成\n模型以及数据准备 !aws s3 cp s3://lhr/bestxl.pt ./ !aws s3 cp s3://lhr/sku.zip ./os2d/ !aws s3 cp s3://lhr/os2d.yaml ./data/ %cd os2d !unzip sku.zip \u0026gt; /dev/null %cd .. !cp /home/ec2-user/SageMaker/xilian/yolov5_sagemaker/test.jpg ./ 模型训练 接下来我们运行训练\n!python train.py --img 640 --batch 16 --epochs 1 --data os2d.yaml --weights bestxl.pt --cache 这里使用我们已经训练好的模型作为训练起点，为了演示目的，我们在640分辨率的输入尺寸下运行一个epoch，大约需要5min\n结果本地测试 我们可以直接用这个产生的模型文件进行本地推理。注意这里的模型文件地址的指定为你刚刚训练产生的。\n!python detect.py --iou-thres 0.6 --conf-thres 0.4 --weights './runs/train/exp2/weights/best.pt' --img 640 --source './test.jpg' Image(filename='runs/detect/exp/test.jpg', width=1000) 输出如下\n到这里，就完成了一个模型的训练过程。\n"},{"uri":"/yolo.html","title":"动手实验1: 基于Amazon SageMaker的YOLOv5模型训练动手实验","tags":[],"description":"","content":"YOLO 是“You only look once”的首字母缩写，是一种将图像划分为网格系统的对象检测算法。网格中的每个单元格负责检测自身内部的对象。由于其速度和准确性，YOLO 是最著名的物体检测算法之一。\n模型架构 YOLO是一个标准的One-Stage目标检测架构，其全称是：You only look once。经过多年的迭代目前已经发布V5版本，起性能和速度均达到业界顶尖水平。 最终预测结果构成为SxS个向量：（类别，置信度，bbox），而后通过后处理算法（NMS）以保留部分结果。\n模型表现 YOLOv5在COCO数据集上的结果同之前的SOTA对比：在所有速度和mAP上均得到了SOTA的性能表现。\nreference  paper： https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html source code：https://github.com/ultralytics/yolov5  Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 779-788.\n"},{"uri":"/os2d/0503deploy.html","title":"Os2d模型部署","tags":[],"description":"","content":"首先打包镜像并推送\n!sh build_and_push.sh os2dv1 然后部署一个预置的endpoint\n#注意修改：015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/yolov5为自己对应的 %cd endpoint !python create_endpoint.py \\ --endpoint_ecr_image_path \u0026#34;015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/os2dv1\u0026#34; \\ --endpoint_name \u0026#39;os2dv1\u0026#39; \\ --instance_type \u0026#34;ml.p3.2xlarge\u0026#34; %cd .. 输出\nmodel_name: yolov5 endpoint_ecr_image_path: 015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/os2dv1 \u0026lt;\u0026lt;\u0026lt; Completed model endpoint deployment. os2dv1 当状态变为InService即代表部署完成\n在部署结束后，看到SageMaker控制台生成了对应的endpoint,可以使用如下客户端代码测试调用\n%%time import boto3 import cv2 import time import cv2 import json import ast import numpy as np body = b\u0026#34;\u0026#34; with open(\u0026#34;./data/demo/5.jpg\u0026#34;, \u0026#34;rb\u0026#34;) as fp: body = fp.read() # body = cv2.imencode(\u0026#34;.jpg\u0026#34;, frame)[1].tobytes() runtime = boto3.client(\u0026#34;sagemaker-runtime\u0026#34;,region_name=\u0026#34;cn-north-1\u0026#34;) response = runtime.invoke_endpoint( EndpointName=\u0026#39;yolov5\u0026#39;, Body=body, ContentType=\u0026#39;application/x-image\u0026#39;, ) body = json.loads(response[\u0026#34;Body\u0026#34;].read().decode()) print(len(body)) print(body) 结果如下 结果如下\n89 ['37 0.100667 0.559 0.0613333 0.119', '31 0.704333 0.37375 0.0566667 0.1375', '39 0.261333 0.21925 0.06 0.1285', '27 0.431667 0.715 0.054 0.13', '6 0.081 0.076 0.062 0.09', '27 0.486333 0.7145 0.054 0.132', '26 0.433 0.5485 0.0566667 0.135', '20 0.486667 0.91 0.056 0.146', '40 0.249333 0.07125 0.048 0.0955', '31 0.646 0.37475 0.056 0.1365', '22 0.211667 0.91325 0.062 0.1465', '17 0.075 0.38875 0.0633333 0.1185', '17 0.137667 0.38775 0.0606667 0.1165', '40 0.297667 0.071 0.0473333 0.095', '26 0.489333 0.54825 0.056 0.1345', '7 0.164667 0.7225 0.0586667 0.126', '10 0.675 0.06625 0.0446667 0.0945', '1 0.540333 0.71475 0.054 0.1315', '18 0.605667 0.21225 0.0566667 0.1385', '5 0.649667 0.90925 0.0566667 0.1475', '36 0.151667 0.90975 0.0566667 0.1545', '42 0.486667 0.068 0.0466667 0.097', '25 0.600333 0.5485 0.0553333 0.131', '16 0.392333 0.07025 0.046 0.0935', '7 0.104667 0.723 0.06 0.124', '25 0.544333 0.54875 0.0553333 0.1325', '5 0.706333 0.908 0.058 0.148', '11 0.364333 0.381 0.0566667 0.128', '16 0.345 0.07125 0.046 0.0945', '1 0.595333 0.714 0.052 0.132', '4 0.140333 0.075 0.0566667 0.09', '42 0.438667 0.069 0.0466667 0.095', '10 0.628667 0.06675 0.0453333 0.0965', '18 0.589333 0.3775 0.0546667 0.128', '18 0.533333 0.3785 0.056 0.128', '43 0.196333 0.3835 0.0553333 0.124', '36 0.0966667 0.9095 0.056 0.155', '15 0.378333 0.5515 0.0553333 0.13', '21 0.762667 0.90775 0.0586667 0.1455', '21 0.820667 0.9075 0.0586667 0.145', '0 0.594 0.90675 0.0533333 0.1525', '33 0.374333 0.21575 0.0553333 0.1365', '18 0.547 0.21275 0.0566667 0.1395', '0 0.541 0.90675 0.0526667 0.1525', '30 0.271333 0.547 0.052 0.143', '4 0.197667 0.0745 0.0553333 0.089', '24 0.819667 0.37225 0.058 0.1335', '20 0.428667 0.90975 0.0573333 0.1485', '15 0.324333 0.5515 0.054 0.13', '30 0.218 0.54725 0.0546667 0.1435', '11 0.309 0.3815 0.0566667 0.128', '37 0.162667 0.55875 0.0626667 0.1185', '34 0.217 0.7175 0.0526667 0.132', '41 0.581333 0.067 0.0466667 0.096', '33 0.316667 0.21625 0.056 0.1365', '28 0.663 0.20875 0.0566667 0.1435', '43 0.254333 0.38375 0.0593333 0.1245', '39 0.201667 0.22025 0.058 0.1265', '38 0.883333 0.54525 0.06 0.1325', '38 0.767 0.5455 0.0553333 0.132', '28 0.721 0.2075 0.058 0.144', '21 0.879 0.90825 0.0606667 0.1475', '41 0.533333 0.06825 0.0466667 0.0945', '34 0.269667 0.71675 0.0526667 0.1315', '38 0.824333 0.54525 0.058 0.1315', '23 0.778333 0.2075 0.058 0.14', '3 0.648667 0.71375 0.0546667 0.1315', '8 0.845667 0.0625 0.100667 0.098', '24 0.878333 0.372 0.062 0.135', '2 0.142 0.2235 0.0613333 0.121', '14 0.421 0.3775 0.054 0.132', '3 0.703 0.71325 0.0553333 0.1315', '13 0.49 0.21275 0.0586667 0.1405', '9 0.747 0.06375 0.098 0.0985', '24 0.762667 0.37325 0.06 0.1365', '2 0.0786667 0.22375 0.0626667 0.1205', '13 0.431 0.21325 0.058 0.1405', '12 0.870667 0.71275 0.0586667 0.1295', '35 0.378 0.715 0.0533333 0.13', '12 0.756667 0.71275 0.056 0.1285', '14 0.478 0.37675 0.056 0.1325', '35 0.323333 0.716 0.0546667 0.131', '35 0.31 0.90625 0.06 0.1555', '12 0.813667 0.7125 0.0566667 0.129', '35 0.369667 0.906 0.058 0.155', '32 0.655333 0.54575 0.0546667 0.1345', '23 0.897333 0.20525 0.06 0.1405', '32 0.711 0.54475 0.0566667 0.1345', '23 0.837667 0.20675 0.0593333 0.1395'] "},{"uri":"/yolo/0503deploy.html","title":"YOLOv5模型部署","tags":[],"description":"","content":"首先打包镜像并推送\n!cp /home/ec2-user/SageMaker/xilian/yolov5/runs/train/exp2/weights/best.pt /home/ec2-user/SageMaker/xilian/yolov5_sagemaker/ %cd /home/ec2-user/SageMaker/xilian/yolov5_sagemaker !sh build_and_push.sh yolov5 然后部署一个预置的endpoint\n#注意修改：015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/yolov5为自己对应的 %cd endpoint !python create_endpoint.py \\ --endpoint_ecr_image_path \u0026#34;015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/yolov5\u0026#34; \\ --endpoint_name \u0026#39;yolov5\u0026#39; \\ --instance_type \u0026#34;ml.p3.2xlarge\u0026#34; %cd .. 输出\nmodel_name: yolov5 endpoint_ecr_image_path: 015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/yolov5 \u0026lt;\u0026lt;\u0026lt; Completed model endpoint deployment. yolov5 当状态变为InService即代表部署完成\n在部署结束后，看到SageMaker控制台生成了对应的endpoint,可以使用如下客户端代码测试调用\n%%time import boto3 import cv2 import time import cv2 import json import ast import numpy as np body = b\u0026#34;\u0026#34; with open(\u0026#34;./test.jpg\u0026#34;, \u0026#34;rb\u0026#34;) as fp: body = fp.read() # body = cv2.imencode(\u0026#34;.jpg\u0026#34;, frame)[1].tobytes() runtime = boto3.client(\u0026#34;sagemaker-runtime\u0026#34;,region_name=\u0026#34;cn-north-1\u0026#34;) response = runtime.invoke_endpoint( EndpointName=\u0026#39;yolov5\u0026#39;, Body=body, ContentType=\u0026#39;application/x-image\u0026#39;, ) body = json.loads(response[\u0026#34;Body\u0026#34;].read().decode()) print(len(body)) print(body) 结果如下\n89 ['37 0.100667 0.559 0.0613333 0.119', '31 0.704333 0.37375 0.0566667 0.1375', '39 0.261333 0.21925 0.06 0.1285', '27 0.431667 0.715 0.054 0.13', '6 0.081 0.076 0.062 0.09', '27 0.486333 0.7145 0.054 0.132', '26 0.433 0.5485 0.0566667 0.135', '20 0.486667 0.91 0.056 0.146', '40 0.249333 0.07125 0.048 0.0955', '31 0.646 0.37475 0.056 0.1365', '22 0.211667 0.91325 0.062 0.1465', '17 0.075 0.38875 0.0633333 0.1185', '17 0.137667 0.38775 0.0606667 0.1165', '40 0.297667 0.071 0.0473333 0.095', '26 0.489333 0.54825 0.056 0.1345', '7 0.164667 0.7225 0.0586667 0.126', '10 0.675 0.06625 0.0446667 0.0945', '1 0.540333 0.71475 0.054 0.1315', '18 0.605667 0.21225 0.0566667 0.1385', '5 0.649667 0.90925 0.0566667 0.1475', '36 0.151667 0.90975 0.0566667 0.1545', '42 0.486667 0.068 0.0466667 0.097', '25 0.600333 0.5485 0.0553333 0.131', '16 0.392333 0.07025 0.046 0.0935', '7 0.104667 0.723 0.06 0.124', '25 0.544333 0.54875 0.0553333 0.1325', '5 0.706333 0.908 0.058 0.148', '11 0.364333 0.381 0.0566667 0.128', '16 0.345 0.07125 0.046 0.0945', '1 0.595333 0.714 0.052 0.132', '4 0.140333 0.075 0.0566667 0.09', '42 0.438667 0.069 0.0466667 0.095', '10 0.628667 0.06675 0.0453333 0.0965', '18 0.589333 0.3775 0.0546667 0.128', '18 0.533333 0.3785 0.056 0.128', '43 0.196333 0.3835 0.0553333 0.124', '36 0.0966667 0.9095 0.056 0.155', '15 0.378333 0.5515 0.0553333 0.13', '21 0.762667 0.90775 0.0586667 0.1455', '21 0.820667 0.9075 0.0586667 0.145', '0 0.594 0.90675 0.0533333 0.1525', '33 0.374333 0.21575 0.0553333 0.1365', '18 0.547 0.21275 0.0566667 0.1395', '0 0.541 0.90675 0.0526667 0.1525', '30 0.271333 0.547 0.052 0.143', '4 0.197667 0.0745 0.0553333 0.089', '24 0.819667 0.37225 0.058 0.1335', '20 0.428667 0.90975 0.0573333 0.1485', '15 0.324333 0.5515 0.054 0.13', '30 0.218 0.54725 0.0546667 0.1435', '11 0.309 0.3815 0.0566667 0.128', '37 0.162667 0.55875 0.0626667 0.1185', '34 0.217 0.7175 0.0526667 0.132', '41 0.581333 0.067 0.0466667 0.096', '33 0.316667 0.21625 0.056 0.1365', '28 0.663 0.20875 0.0566667 0.1435', '43 0.254333 0.38375 0.0593333 0.1245', '39 0.201667 0.22025 0.058 0.1265', '38 0.883333 0.54525 0.06 0.1325', '38 0.767 0.5455 0.0553333 0.132', '28 0.721 0.2075 0.058 0.144', '21 0.879 0.90825 0.0606667 0.1475', '41 0.533333 0.06825 0.0466667 0.0945', '34 0.269667 0.71675 0.0526667 0.1315', '38 0.824333 0.54525 0.058 0.1315', '23 0.778333 0.2075 0.058 0.14', '3 0.648667 0.71375 0.0546667 0.1315', '8 0.845667 0.0625 0.100667 0.098', '24 0.878333 0.372 0.062 0.135', '2 0.142 0.2235 0.0613333 0.121', '14 0.421 0.3775 0.054 0.132', '3 0.703 0.71325 0.0553333 0.1315', '13 0.49 0.21275 0.0586667 0.1405', '9 0.747 0.06375 0.098 0.0985', '24 0.762667 0.37325 0.06 0.1365', '2 0.0786667 0.22375 0.0626667 0.1205', '13 0.431 0.21325 0.058 0.1405', '12 0.870667 0.71275 0.0586667 0.1295', '35 0.378 0.715 0.0533333 0.13', '12 0.756667 0.71275 0.056 0.1285', '14 0.478 0.37675 0.056 0.1325', '35 0.323333 0.716 0.0546667 0.131', '35 0.31 0.90625 0.06 0.1555', '12 0.813667 0.7125 0.0566667 0.129', '35 0.369667 0.906 0.058 0.155', '32 0.655333 0.54575 0.0546667 0.1345', '23 0.897333 0.20525 0.06 0.1405', '32 0.711 0.54475 0.0566667 0.1345', '23 0.837667 0.20675 0.0593333 0.1395'] "},{"uri":"/os2d.html","title":"动手实验2: 基于Amazon SageMaker的Os2d模型使用和部署 ","tags":[],"description":"","content":"OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features,一种仅需要一张标注图像即可实现目标检测的模型，在完全不在客户数据集funeturn的情况下达到近乎完美的效果。\nPS：需要特定的光照，拍摄距离以及拍摄角度才能达到与YOLO媲美的性能\n模型架构 Os2d是基于One-shot Learning的One-Stage目标检测算法，其中心思想类似于以图搜图，只不过在这里以图搜图换成了以特征搜特征。其模型主要分为四个模块：\n 特征提取模块：通过ResNet作为backbone对类别图像和输入图像分别进行特征编码得到两组feature map。 特征匹配模块：将第一步提出的两组feature在特征空间上计算相似度，得到相似度特征矩阵。 空域增强模块：通过TransformNet对相似度特征矩阵生成仿射变换参数，以适应图像失真以及不同角度带来的形变问题。 目标检测模块：协同相似度特征矩阵和仿射变换参数矩阵生成最终的检测结果（检测框位置+置信度+类别）。  模型表现 模型在各种数据集划分下的表现：测试集中的目标没有在训练集里出现过，可以认为是另一种程度上的zero-shot。\nreference  paper： https://arxiv.org/abs/2003.06800 source code：https://github.com/aosokin/os2d  Osokin A , Sumin D , Lomakin V . OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features[J]. ECCV 2020.\n"},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""}]